### use # to comment out the configure item

### I/O ###
train_dir=/data/rsg/nlp/juanmoo1/projects/02_takeda_dev/00_takeda/02_workdir_takeda/02_pilot/10_data/oversampled/train.conll
dev_dir=/data/rsg/nlp/juanmoo1/projects/02_takeda_dev/00_takeda/02_workdir_takeda/02_pilot/10_data/oversampled/valid.conll
test_dir=/data/rsg/nlp/juanmoo1/projects/02_takeda_dev/00_takeda/02_workdir_takeda/02_pilot/10_data/oversampled/valid.conll
model_dir=/data/rsg/nlp/juanmoo1/projects/02_takeda_dev/00_takeda/01_takeda_repo/NCRFpp/trained_models/bilstm.model
word_emb_dir=/data/rsg/nlp/juanmoo1/projects/02_takeda_dev/00_takeda/01_takeda_repo/NCRFpp/pubmed.embeddings.txt

#raw_dir=
#decode_dir=
#dset_dir=
#load_model_dir=
#char_emb_dir=

norm_word_emb=False
norm_char_emb=False
number_normalized=True
seg=True
word_emb_dim=50
char_emb_dim=30

###NetworkConfiguration###
use_crf=True
use_char=True
word_seq_feature=LSTM
char_seq_feature=CNN
#feature=[POS] emb_size=20
#feature=[Cap] emb_size=20
#nbest=1

###TrainingSetting###
status=train
optimizer=SGD
iteration=250
batch_size=20
ave_batch_loss=True

###Hyperparameters###
cnn_layer=4
char_hidden_dim=50
hidden_dim=200
dropout=0.5
lstm_layer=1
bilstm=True
#learning_rate=0.02
learning_rate=0.02
lr_decay=0.005
momentum=0
l2=1e-8
gpu = True
